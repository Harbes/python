{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Handling Numerical Data\n",
    "* scaling的方法：min-max、standard\n",
    "## 4.1 rescaling a feature: rescale the values of a numerical feature to be between two values.（适合神经网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.28579586]\n",
      " [0.35724483]\n",
      " [0.42869379]\n",
      " [1.        ]]\n",
      "[[-500.5]\n",
      " [-100.1]\n",
      " [   0. ]\n",
      " [ 100.1]\n",
      " [ 900.5]]\n"
     ]
    }
   ],
   "source": [
    "# 采用的是 min-max scaling方法\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "#feature = np.array([-500.5,-100.1,0.0,100.1,500.5]) # 需要输入 2D-array\n",
    "feature = np.array([\n",
    "    [-500.5],\n",
    "    [-100.1],\n",
    "    [0.0],\n",
    "    [100.1],\n",
    "    [900.5]\n",
    "])\n",
    "\n",
    "# create scaler\n",
    "minmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Scale feature\n",
    "scaled_feature=minmax_scale.fit_transform(feature)\n",
    "print(scaled_feature)\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 standardizing a feature: transform a feature to have a mean of 0 and a standard deviation of 1.（适合主成分分析）\n",
    "* 注意：当存在异常值时，建议 suing the median and quantile range(取代均值和标准差)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.58713949],\n",
       "       [-0.56345921],\n",
       "       [ 0.33302878],\n",
       "       [ 0.46111678],\n",
       "       [ 1.35645314]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "x=np.array((\n",
    "[-1000.1],\n",
    "[-200.1],\n",
    "[500.5],\n",
    "[600.6],\n",
    "[1300.3]))\n",
    "\n",
    "# create a scaler\n",
    "scaler=preprocessing.StandardScaler()\n",
    "\n",
    "# fit & transform\n",
    "standardized=scaler.fit_transform(x)\n",
    "standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87411015],\n",
       "       [-0.87498439],\n",
       "       [ 0.        ],\n",
       "       [ 0.12501561],\n",
       "       [ 0.99887598]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scaler\n",
    "robust_scaler=preprocessing.RobustScaler()\n",
    "# transform\n",
    "robust_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Normalizing Observation: rescale the feature values of observations to have unit norm(a total length of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5       ,  0.5       ],\n",
       "       [ 0.06912442,  0.93087558],\n",
       "       [ 0.04524008,  0.95475992],\n",
       "       [ 0.76760563,  0.23239437]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "features=np.array([\n",
    "    [-0.5,0.5],\n",
    "    [1.5,20.2],\n",
    "    [1.63,34.4],\n",
    "    [10.9,3.3]\n",
    "])\n",
    "\n",
    "# create normalizer\n",
    "normalizer=Normalizer(norm=\"l2\") # \"2\"表示欧几里得范数中的次数是2\n",
    "\n",
    "# transform\n",
    "normalizer.transform(features) # 每一行的norm为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 _Generating_ Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  4.,  6.,  9.],\n",
       "       [ 4.,  5., 16., 20., 25.],\n",
       "       [ 6.,  7., 36., 42., 49.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "features = np.array([[2, 3], [4, 5], [6, 7]])\n",
    "\n",
    "# create PolynomialFeatures object\n",
    "polynomial_interaction= PolynomialFeatures(degree=2,include_bias=False) # 2是多项式的最高次\n",
    "\n",
    "# create polynomial features\n",
    "polynomial_interaction.fit_transform(features) # 两个一次项，两个平方项以及一个交叉项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  6.],\n",
       "       [ 4.,  5., 20.],\n",
       "       [ 6.,  7., 42.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create PolynomialFeatures object with only interaction features by setting \"interaction_only\" to True\n",
    "interaction = PolynomialFeatures(degree=2, \n",
    "                                 interaction_only=True, \n",
    "                                 include_bias=False) \n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Transforming Features: make a custom transformation to one or more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [14, 15],\n",
       "       [16, 17]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "features = np.array([[2, 3], [4, 5], [6, 7]])\n",
    "\n",
    "# Define a simple function\n",
    "def add_ten(x):\n",
    "    return x+10\n",
    "\n",
    "# create transformer\n",
    "ten_transformer=FunctionTransformer(add_ten)\n",
    "\n",
    "ten_transformer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2\n",
       "0        12        13\n",
       "1        14        15\n",
       "2        16        17"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the same transfromation in pandas using apply\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(features,columns=[\"feature1\",\"feature2\"])\n",
    "# apply function\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Detecting Outliers: identify extreme observations\n",
    "* 方法一：EllipticEnvelope-> 此方法的一个局限性在于：需要实现设定一个 contamination(污染)参数，即观测值中outlier的比例\n",
    "* 方法二：IQR-based detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+04  3.52863145e+00]\n",
      " [ 1.00000000e+04  5.55121358e+00]\n",
      " [-1.61734616e+00  4.98930508e+00]\n",
      " [-5.25790464e-01  3.30659860e+00]\n",
      " [ 8.52518583e-02  3.64528297e+00]\n",
      " [-7.94152277e-01  2.10495117e+00]\n",
      " [-1.34052081e+00  4.15711949e+00]\n",
      " [-1.98197711e+00  4.02243551e+00]\n",
      " [-2.18773166e+00  3.33352125e+00]\n",
      " [-1.97451969e-01  2.34634916e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# create simuylated data\n",
    "features,_=make_blobs(n_samples=10,n_features=2,centers=1,random_state=1)\n",
    "\n",
    "# replace the first observation's values with extreme values\n",
    "features[0,0]=10000\n",
    "features[1,0]=10000\n",
    "\n",
    "# create detector\n",
    "outlier_detector=EllipticEnvelope(contamination=.1)\n",
    "\n",
    "# fit detector\n",
    "outlier_detector.fit(features)\n",
    "\n",
    "# predict outliers\n",
    "print(features)\n",
    "outlier_detector.predict(features) # 1表示 inlier，-1 表示outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+04  1.00000000e+04 -1.61734616e+00 -5.25790464e-01\n",
      "  8.52518583e-02 -7.94152277e-01 -1.34052081e+00 -1.98197711e+00\n",
      " -2.18773166e+00 -1.97451969e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using interquartile range to identify extreme values\n",
    "feature=features[:,0]\n",
    "\n",
    "# create a function to return index of outliers\n",
    "def indices_of_outliers(x):\n",
    "    q1,q3=np.percentile(x,[25,75])\n",
    "    iqr=q3-q1\n",
    "    lower_bound=q1-iqr*1.5\n",
    "    upper_bound=q3+iqr*1.5\n",
    "    return np.where((x>upper_bound) | (x< lower_bound))\n",
    "\n",
    "print(feature)\n",
    "indices_of_outliers(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>outlier</th>\n",
       "      <th>log_of_square_feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.778956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  outlier  log_of_square_feet\n",
       "0   524433        2.0         1500        0            7.313220\n",
       "1   392333        3.5         2500        0            7.824046\n",
       "2   293222        2.0         1500        0            7.313220\n",
       "3  4322032      116.0        48000        1           10.778956"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "houses = pd.DataFrame()\n",
    "houses['Price']=[524433,392333,293222,4322032]\n",
    "houses['Bathrooms']=[2,3.5,2,116]\n",
    "houses['Square_Feet']=[1500,2500,1500,48000]\n",
    "\n",
    "#Filter observations\n",
    "houses[houses['Bathrooms']<20]\n",
    "\n",
    "# 接下来可以 mark them as outliers and include it as a feature\n",
    "\n",
    "import numpy as np\n",
    "# create feature based on boolean condition\n",
    "houses[\"outlier\"]=np.where(houses['Bathrooms']<20,0,1)\n",
    "houses\n",
    "\n",
    "# 最后，we can transfrom the feature to dampen the effect of the outlier(取对数)\n",
    "houses['log_of_square_feet']=[np.log(x) for x in houses['Square_Feet']]\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Discretizating Features：have a numerical feature and want to break it up into discrete bins\n",
    "* 方法一：binarize the feature according to some threshold(单一的threshold)\n",
    "* 方法二：..................................multiple ........\n",
    "\n",
    "* Tips: Discretization can be a fruitful strategy when we have reason to believe that a numerical feature should behave more like a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "age = np.array([[6], [12], [20], [36], [65]]) \n",
    " \n",
    "# create binarizer\n",
    "binarizer=Binarizer(36)\n",
    "\n",
    "#transform feature\n",
    "binarizer.fit_transform(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break up numerical features according to multiple threshold:\n",
    "np.digitize(age,bins=[20,30,64]) # 每一个数字代表bin的right edge（不包含在内）\n",
    "# 也可以通过设置参数 “right=True”来包含右边界\n",
    "np.digitize(age,bins=[20,30,64],right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Grouping Observations Using Clustering: cluster observations so that similar observations are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.794152</td>\n",
       "      <td>2.104951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2.760179</td>\n",
       "      <td>5.551214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.946905</td>\n",
       "      <td>-4.590344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.525790</td>\n",
       "      <td>3.306599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.981977</td>\n",
       "      <td>4.022436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-5.865964</td>\n",
       "      <td>-7.968072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-6.834787</td>\n",
       "      <td>-7.391217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-6.749247</td>\n",
       "      <td>-10.175429</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-10.752110</td>\n",
       "      <td>-2.700480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-8.508996</td>\n",
       "      <td>-8.657694</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.330806</td>\n",
       "      <td>4.393825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.197452</td>\n",
       "      <td>2.346349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.085252</td>\n",
       "      <td>3.645283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-10.206607</td>\n",
       "      <td>-3.366725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-9.158729</td>\n",
       "      <td>-3.022246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.340521</td>\n",
       "      <td>4.157119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.831988</td>\n",
       "      <td>3.528631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-9.806797</td>\n",
       "      <td>-1.853093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.758704</td>\n",
       "      <td>3.722762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-11.140231</td>\n",
       "      <td>-4.302691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-7.812137</td>\n",
       "      <td>-5.349845</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-2.351221</td>\n",
       "      <td>4.009736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-6.878321</td>\n",
       "      <td>-7.743176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.782450</td>\n",
       "      <td>3.470720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-7.371086</td>\n",
       "      <td>-7.325253</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-7.735544</td>\n",
       "      <td>-7.775664</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-11.115023</td>\n",
       "      <td>-3.718933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-9.697542</td>\n",
       "      <td>-4.305598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-10.189548</td>\n",
       "      <td>-4.840978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-2.187732</td>\n",
       "      <td>3.333521</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-2.346733</td>\n",
       "      <td>3.561284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.927448</td>\n",
       "      <td>4.936845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-10.744871</td>\n",
       "      <td>-2.260894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-6.866582</td>\n",
       "      <td>-8.034219</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-7.512011</td>\n",
       "      <td>-6.928720</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-6.904845</td>\n",
       "      <td>-7.277059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-1.617346</td>\n",
       "      <td>4.989305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.757969</td>\n",
       "      <td>4.908984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-9.484783</td>\n",
       "      <td>-4.251441</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-7.408736</td>\n",
       "      <td>-8.109631</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-9.509194</td>\n",
       "      <td>-4.028920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-8.337910</td>\n",
       "      <td>-3.211304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-9.712125</td>\n",
       "      <td>-3.068207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-8.866083</td>\n",
       "      <td>-2.433532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-7.684883</td>\n",
       "      <td>-7.455196</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1   feature2  group\n",
       "0   -9.877554  -3.336145      0\n",
       "1   -7.287210  -8.353986      2\n",
       "2   -6.943061  -7.023744      2\n",
       "3   -7.440167  -8.791959      2\n",
       "4   -6.641388  -8.075888      2\n",
       "5   -0.794152   2.104951      1\n",
       "6   -2.760179   5.551214      1\n",
       "7   -9.946905  -4.590344      0\n",
       "8   -0.525790   3.306599      1\n",
       "9   -1.981977   4.022436      1\n",
       "10  -5.865964  -7.968072      2\n",
       "11  -6.834787  -7.391217      2\n",
       "12  -6.749247 -10.175429      2\n",
       "13 -10.752110  -2.700480      0\n",
       "14  -8.508996  -8.657694      2\n",
       "15  -2.330806   4.393825      1\n",
       "16  -0.197452   2.346349      1\n",
       "17   0.085252   3.645283      1\n",
       "18 -10.206607  -3.366725      0\n",
       "19  -9.158729  -3.022246      0\n",
       "20  -1.340521   4.157119      1\n",
       "21  -1.831988   3.528631      1\n",
       "22  -9.806797  -1.853093      0\n",
       "23  -0.758704   3.722762      1\n",
       "24 -11.140231  -4.302691      0\n",
       "25  -7.812137  -5.349845      2\n",
       "26  -2.351221   4.009736      1\n",
       "27  -6.878321  -7.743176      2\n",
       "28  -1.782450   3.470720      1\n",
       "29  -7.371086  -7.325253      2\n",
       "30  -7.735544  -7.775664      2\n",
       "31 -11.115023  -3.718933      0\n",
       "32  -9.697542  -4.305598      0\n",
       "33 -10.189548  -4.840978      0\n",
       "34  -2.187732   3.333521      1\n",
       "35  -2.346733   3.561284      1\n",
       "36  -1.927448   4.936845      1\n",
       "37 -10.744871  -2.260894      0\n",
       "38  -6.866582  -8.034219      2\n",
       "39  -7.512011  -6.928720      2\n",
       "40  -6.904845  -7.277059      2\n",
       "41  -1.617346   4.989305      1\n",
       "42  -0.757969   4.908984      1\n",
       "43  -9.484783  -4.251441      0\n",
       "44  -7.408736  -8.109631      2\n",
       "45  -9.509194  -4.028920      0\n",
       "46  -8.337910  -3.211304      0\n",
       "47  -9.712125  -3.068207      0\n",
       "48  -8.866083  -2.433532      0\n",
       "49  -7.684883  -7.455196      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# make simulated feature matrix\n",
    "features,_=make_blobs(n_samples=50,\n",
    "                     n_features=2,\n",
    "                     centers=3,\n",
    "                     random_state=1)\n",
    "# create dataframe\n",
    "df=pd.DataFrame(features,columns=[\"feature1\",\"feature2\"])\n",
    "# make k-means clusterer\n",
    "clusterer=KMeans(3,random_state=0)\n",
    "# fit clusterer\n",
    "clusterer.fit(features)\n",
    "# predict values\n",
    "df['group']=clusterer.predict(features)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Deleting Observations with Missing Values\n",
    "* 许多 machine learning algorithms 无法处理 missing values，所以需要在preprocessing阶段解决missing value的问题\n",
    "* 有三种类型的missing values：missing completely at random、missing at random、missing not at random。如果是最后一种情况，那么deleting missing values 可能会引入bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 11.1],\n",
       "       [ 2.2, 22.2],\n",
       "       [ 3.3, 33.3],\n",
       "       [ 4.4, 44.4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "features = np.array([[1.1, 11.1],\n",
    "                     [2.2, 22.2],\n",
    "                     [3.3, 33.3],\n",
    "                     [4.4, 44.4],\n",
    "                     [np.nan, 55]])\n",
    "# keep only observations that are not missing\n",
    "features[~np.isnan(features).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2\n",
       "0       1.1      11.1\n",
       "1       2.2      22.2\n",
       "2       3.3      33.3\n",
       "3       4.4      44.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatively, we can drop missing observations using pandas\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(features, columns=[\"feature1\",\"feature2\"])\n",
    "# remove observations with missing values\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 imputing missing values: you have missing values in your data and want to fill in or predict their values\n",
    "* 一般来说，有两种策略可以用于填补missing values，方法一是使用machine learning取predict，例如KNN；另一种就是简单地使用某个平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 0.8730186113995938\n",
      "imputed value 1.0955332713113226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fancyimpute import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features,_=make_blobs(n_samples=1000,\n",
    "                     n_features=2,\n",
    "                     random_state=1)\n",
    "\n",
    "# standardize the features\n",
    "scaler=StandardScaler()\n",
    "standardized_features=scaler.fit_transform(features)\n",
    "\n",
    "# replace the first feature's first values with a missing value\n",
    "true_value=standardized_features[0,0]\n",
    "standardized_features[0,0]=np.nan\n",
    "\n",
    "# predict the missing values in the feature matrix\n",
    "features_knn_imputed=KNN(k=5,verbose=0).complete(standardized_features)\n",
    "\n",
    "# compare true and imputed values\n",
    "print('True value:',true_value)\n",
    "print(\"imputed value\",features_knn_imputed[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 0.8730186113995938\n",
      "imputed value -3.058372724614996\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can use scikit-learn's Imputer module to fill in missing values with the feature's mean, median, or most frequent value.\n",
    "# However, we will typically get worse results than KNN:\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "# create imputer\n",
    "mean_imputer=Imputer(strategy='mean',axis=0)\n",
    "# impute value\n",
    "features_mean_imputed=mean_imputer.fit_transform(features)\n",
    "# compare true and imputed values\n",
    "print('True value:',true_value)\n",
    "print(\"imputed value\",features_mean_imputed[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Handling Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Encoding Nominal Categorical Features: have a feature with nominal calssed that has no intrinsic ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "feature = np.array([[\"Texas\"],\n",
    "                    [\"California\"],\n",
    "                    [\"Texas\"],\n",
    "                    [\"Delaware\"],\n",
    "                    [\"Texas\"]])\n",
    "# create one-hot encoder\n",
    "one_hot=LabelBinarizer()\n",
    "# one_hot encode feature\n",
    "one_hot.fit_transform(feature) # 从结果来看，原理应该是将其影射在多维空间上，有多少种类，就有多少维度\n",
    "# if we want to reverse the one-hot encoding, we can use \"inverse_transform\"\n",
    "one_hot.inverse_transform(one_hot.fit_transform(feature))\n",
    "# we can use the \"classes_\" method to output the classes\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>California</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Texas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   California  Delaware  Texas\n",
       "0           0         0      1\n",
       "1           1         0      0\n",
       "2           0         0      1\n",
       "3           0         1      0\n",
       "4           0         0      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can even use pandas to one-hot encode the feature\n",
    "import pandas as pd\n",
    "# create dummy variable from feature\n",
    "pd.get_dummies(feature[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One helpful ability of scikit-learn is to handle a situation where each observation lists multiple classes, 例如\n",
    "multiclass_feature = [(\"Texas\", \"Florida\"), \n",
    "                      (\"California\", \"Alabama\"), \n",
    "                      (\"Texas\", \"Florida\"),\n",
    "                      (\"Delware\", \"Florida\"),\n",
    "                      (\"Texas\", \"Alabama\")] \n",
    "# create multiclass one-hot encoder\n",
    "one_hot_multiclass=MultiLabelBinarizer()\n",
    "#one-hot encode multiclass feature\n",
    "one_hot_multiclass.fit_transform(multiclass_feature)\n",
    "# view classes\n",
    "one_hot_multiclass.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Encoding Ordinal Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})\n",
    "# create mapper\n",
    "scale_mapper = {\"Low\":1,\n",
    "                \"Medium\":2,\n",
    "                \"High\":3} # 注意，有时候数值选择很重要；可以选择小数\n",
    "# replace feature values with scale\n",
    "dataframe['Score'].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Encoding Dictionaries of Features: you have a dictionary and want to convert it into a feature matrix\n",
    "* 需要额外关注 dictvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "             {\"Red\": 4, \"Blue\": 3},\n",
    "             {\"Red\": 1, \"Yellow\": 2},\n",
    "             {\"Red\": 2, \"Yellow\": 2}]\n",
    "# create dictionary vectorizer\n",
    "dictvectorizer=DictVectorizer(sparse=False)\n",
    "# convert dictionary to feature matirx\n",
    "features=dictvectorizer.fit_transform(data_dict)\n",
    "pd.DataFrame(features,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can get the names of each generated feature using the get_feature_names method:\n",
    "\n",
    "feature_names = dictvectorizer.get_feature_names() \n",
    "# View feature names \n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Imputing Missing Class Values: you have a categorical feature missing value that you want to replace with predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create feature matrix with categorical feature\n",
    "X = np.array([[0, 2.10, 1.45],\n",
    "              [1, 1.18, 1.33],\n",
    "              [0, 1.22, 1.27],\n",
    "              [1, -0.21, -1.19]])\n",
    "\n",
    "# Create feature matrix with missing values in the categorical feature\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "                       [np.nan, -0.67, -0.22]])\n",
    "# train KNN learner\n",
    "clf=KNeighborsClassifier(3,weights=\"distance\")\n",
    "trained_model=clf.fit(X[:,1:],X[:,0])\n",
    "# predict missing values' class\n",
    "imputed_values=trained_model.predict(X_with_nan[:,1:])\n",
    "# join column of predicted class with their other features\n",
    "X_with_imputed=np.hstack((imputed_values.reshape(-1,1),X_with_nan[:,1:]))\n",
    "# join two feature matrices\n",
    "np.vstack((X_with_imputed,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 0.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An alternative solution is to fill in missing values with the feature’s most frequent value:\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "# join the two feature matrices\n",
    "X_complete=np.vstack((X_with_nan,X))\n",
    "imputer=Imputer(strategy=\"most_frequent\",axis=0)\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Handling Imbalanced Classes: have a vector with highly imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load iris data\n",
    "iris= load_iris()\n",
    "\n",
    "# create feature matirx\n",
    "features=iris.data\n",
    "# create target vector\n",
    "target=iris.target\n",
    "# remove first 40 observations\n",
    "features=features[40:,:]\n",
    "target=target[40:]\n",
    "# create binary target vector indicating if class 0\n",
    "target=np.where((target==0),0,1)\n",
    "# look at the imbalanced target vector\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 0.9, 1: 0.1},\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many algorithms in scikit-learn offer a parameter to weight classes during training to counteract the effect of their imbalance.\n",
    "\n",
    "weights={0:0.9,1:0.1}\n",
    "# create random forest classifier with weights\n",
    "RandomForestClassifier(class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can pass balanced, which automatically creates weights inversely proportional to class frequencies: \n",
    "\n",
    "# train a random foprest with balanced class weights\n",
    "RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Handling Text\n",
    "## 6.1 Cleaning Text: complete some basic cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = [\"   Interrobang. By Aishwarya Henriette     \",\n",
    "             \"Parking And Going. By Karl Gautier\",\n",
    "             \"    Today Is The night. By Jarek Prakash   \"]\n",
    "# strip whitespaces：去除文本前后的空格\n",
    "strip_whitespaces=[string.strip() for string in text_data]\n",
    "strip_whitespaces\n",
    "# remove periods\n",
    "remove_periods=[string.replace('.','') for string in strip_whitespaces]\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also create and apply a custom transformation function\n",
    "\n",
    "def capitalizer(string:str)->str:\n",
    "    return string.upper()\n",
    "# apply function\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can use regular expressions to make powerful string operations\n",
    "\n",
    "import re\n",
    "def replace_letters_with_X(string:str)->str:\n",
    "    return re.sub(r\"[a-zA-Z]\",'X',string)\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Parsing and Cleaning HTML: you have text data with HTML elements and want to extract just the text\n",
    "* use Beautiful Soup's extensive set of options to parse and extract from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n       Masego Azra'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# create some HTML code\n",
    "html =\"\"\"\n",
    "       <div class='full_name'><span style='font-weight:bold'>\n",
    "       Masego</span> Azra</div>\"\n",
    "       \"\"\"\n",
    "# parse HTML\n",
    "soup=BeautifulSoup(html,'lxml')\n",
    "# find the div with the class \"full_name\", show text\n",
    "soup.find(\"div\",{\"class\":\"full_name\"}).text # 结果有一点偏差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Removing Punctuation\n",
    "* 关注 str.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '10000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "# create a dictionary of punctuation characters\n",
    "punctuation=dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                         if unicodedata.category(chr(i)).startswith('P'))\n",
    "# for each string, remove any pnctuatioin characters\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Tokenizing Text: break text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "# Tokenize words\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also tokenize into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "# tokenize sentences\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Removing Stop Words: Given tokenized text data, you want to remove extremely common words(e,g,, a, is, of, on) that contain little informational value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'go',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "# load stop words\n",
    "stop_words=stopwords.words('english') # ，没发现支持中文？？？\n",
    "# remove stop words\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Stemming Words: you have tokenized words and want to convert them into their root froms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting'] \n",
    "# create stemmer\n",
    "porter=PorterStemmer()\n",
    "# allpy stemmer\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Tagging parts of speech: you have text data and want to tag each word or character with its part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "text_data=\"Chris loved outdoor running\"\n",
    "# use pre-trained part of speech tagger\n",
    "text_tagged=pos_tag(word_tokenize(text_data))\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once the text has been tagged, we can use the tags to find certain parts of speech. For example, here are all nouns:\n",
    "\n",
    "# filter words\n",
    "[word for word,tag in text_tagged if tag in ['NN','NNS','NNP',\"NNPS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A more realistic situation would be that we have data where every observation \n",
    "# contains a tweet and we want to convert those sentences into features for \n",
    "# individual parts of speech (e.g., a feature with 1 if a proper noun is present, and 0 otherwise):\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "# create list\n",
    "tagged_tweets=[]\n",
    "# tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag=pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word,tag in tweet_tag])\n",
    "# use one-hot encoding to convert the tags into features\n",
    "one_hot_multi=MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Encoding Text as a Bag of Words: you have text data and want to create a set of features indicating the number of times an observation's text contains a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "# create the bag of words feature matrix\n",
    "count=CountVectorizer()\n",
    "bag_of_words=count.fit_transform(text_data)\n",
    "bag_of_words\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we  can use the \"vocabulary_\" method to view the word associated with each feature\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [0]\n",
      " [0]]\n",
      "{'brazil': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# View feature matrix\n",
    "print(bag.toarray())\n",
    "# View the 1-grams and 2-grams\n",
    "print(count_2gram.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 Weighting Word Importance: you want a bag of word, but with words weighted by their importance to an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both']) \n",
    "tfidf=TfidfVectorizer()\n",
    "feature_matrix=tfidf.fit_transform(text_data)\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show tf-idf feature matrix as dense matrix\n",
    "feature_matrix.toarray()\n",
    "# show feature names\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capter 7 Handling Dates and Times\n",
    "## 7.1 Converting Strings to Dates: given a vector of strings representing dates and times, you want to transform them into time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2005-04-03 23:35:00'),\n",
       " Timestamp('2010-05-23 00:01:00'),\n",
       " Timestamp('2009-09-04 21:09:00')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "date_strings = np.array(['03-04-2005 11:35 PM',\n",
    "                         '23-05-2010 12:01 AM',\n",
    "                         '04-09-2009 09:09 PM'])\n",
    "# convert to datetime\n",
    "[pd.to_datetime(date,format='%d-%m-%Y %I:%M %p') for date in date_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2005-04-03 23:35:00'),\n",
       " Timestamp('2010-05-23 00:01:00'),\n",
       " Timestamp('2009-09-04 21:09:00')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we might also want to add an argument to the  \"errors\"  parameter to handle problems\n",
    "\n",
    "[pd.to_datetime(date,format='%d-%m-%Y %I:%M %p',errors='coerce') for date in date_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Handling Time Zones: if not specified, pandas objects have no time zone. However,  we can add a time zone using \"tz\" during creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-05-01 06:00:00+0100', tz='Europe/London')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Timestamp('2017-05-01 06:00:00',tz='Europe/London')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-05-01 06:00:00+0100', tz='Europe/London')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can add a time zone to a previously created datetime using tz_localize:\n",
    "date=pd.Timestamp('2017-05-01 06:00:00')\n",
    "# set time zone\n",
    "date_in_london=date.tz_localize('Europe/London')\n",
    "date_in_london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-05-01 05:00:00+0000', tz='Africa/Abidjan')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  we can also convert to a different time zone\n",
    "\n",
    "date_in_london.tz_convert('Africa/Abidjan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2002-02-28 00:00:00+00:00\n",
       "1   2002-03-31 00:00:00+00:00\n",
       "2   2002-04-30 00:00:00+00:00\n",
       "dtype: datetime64[ns, Africa/Abidjan]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, pandas' Series objects can apply \"tz_localize\" and \"tz_convert\" to every element\n",
    "\n",
    "dates=pd.Series(pd.date_range('2/2/2002',periods=3,freq='M'))\n",
    "# set time zone\n",
    "dates.dt.tz_localize('Africa/Abidjan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Africa/Abidjan',\n",
       " 'Africa/Accra',\n",
       " 'Africa/Addis_Ababa',\n",
       " 'Africa/Algiers',\n",
       " 'Africa/Asmara',\n",
       " 'Africa/Asmera',\n",
       " 'Africa/Bamako',\n",
       " 'Africa/Bangui',\n",
       " 'Africa/Banjul',\n",
       " 'Africa/Bissau',\n",
       " 'Africa/Blantyre',\n",
       " 'Africa/Brazzaville',\n",
       " 'Africa/Bujumbura',\n",
       " 'Africa/Cairo',\n",
       " 'Africa/Casablanca',\n",
       " 'Africa/Ceuta',\n",
       " 'Africa/Conakry',\n",
       " 'Africa/Dakar',\n",
       " 'Africa/Dar_es_Salaam',\n",
       " 'Africa/Djibouti',\n",
       " 'Africa/Douala',\n",
       " 'Africa/El_Aaiun',\n",
       " 'Africa/Freetown',\n",
       " 'Africa/Gaborone',\n",
       " 'Africa/Harare',\n",
       " 'Africa/Johannesburg',\n",
       " 'Africa/Juba',\n",
       " 'Africa/Kampala',\n",
       " 'Africa/Khartoum',\n",
       " 'Africa/Kigali',\n",
       " 'Africa/Kinshasa',\n",
       " 'Africa/Lagos',\n",
       " 'Africa/Libreville',\n",
       " 'Africa/Lome',\n",
       " 'Africa/Luanda',\n",
       " 'Africa/Lubumbashi',\n",
       " 'Africa/Lusaka',\n",
       " 'Africa/Malabo',\n",
       " 'Africa/Maputo',\n",
       " 'Africa/Maseru',\n",
       " 'Africa/Mbabane',\n",
       " 'Africa/Mogadishu',\n",
       " 'Africa/Monrovia',\n",
       " 'Africa/Nairobi',\n",
       " 'Africa/Ndjamena',\n",
       " 'Africa/Niamey',\n",
       " 'Africa/Nouakchott',\n",
       " 'Africa/Ouagadougou',\n",
       " 'Africa/Porto-Novo',\n",
       " 'Africa/Sao_Tome',\n",
       " 'Africa/Timbuktu',\n",
       " 'Africa/Tripoli',\n",
       " 'Africa/Tunis',\n",
       " 'Africa/Windhoek',\n",
       " 'America/Adak',\n",
       " 'America/Anchorage',\n",
       " 'America/Anguilla',\n",
       " 'America/Antigua',\n",
       " 'America/Araguaina',\n",
       " 'America/Argentina/Buenos_Aires',\n",
       " 'America/Argentina/Catamarca',\n",
       " 'America/Argentina/ComodRivadavia',\n",
       " 'America/Argentina/Cordoba',\n",
       " 'America/Argentina/Jujuy',\n",
       " 'America/Argentina/La_Rioja',\n",
       " 'America/Argentina/Mendoza',\n",
       " 'America/Argentina/Rio_Gallegos',\n",
       " 'America/Argentina/Salta',\n",
       " 'America/Argentina/San_Juan',\n",
       " 'America/Argentina/San_Luis',\n",
       " 'America/Argentina/Tucuman',\n",
       " 'America/Argentina/Ushuaia',\n",
       " 'America/Aruba',\n",
       " 'America/Asuncion',\n",
       " 'America/Atikokan',\n",
       " 'America/Atka',\n",
       " 'America/Bahia',\n",
       " 'America/Bahia_Banderas',\n",
       " 'America/Barbados',\n",
       " 'America/Belem',\n",
       " 'America/Belize',\n",
       " 'America/Blanc-Sablon',\n",
       " 'America/Boa_Vista',\n",
       " 'America/Bogota',\n",
       " 'America/Boise',\n",
       " 'America/Buenos_Aires',\n",
       " 'America/Cambridge_Bay',\n",
       " 'America/Campo_Grande',\n",
       " 'America/Cancun',\n",
       " 'America/Caracas',\n",
       " 'America/Catamarca',\n",
       " 'America/Cayenne',\n",
       " 'America/Cayman',\n",
       " 'America/Chicago',\n",
       " 'America/Chihuahua',\n",
       " 'America/Coral_Harbour',\n",
       " 'America/Cordoba',\n",
       " 'America/Costa_Rica',\n",
       " 'America/Creston',\n",
       " 'America/Cuiaba',\n",
       " 'America/Curacao',\n",
       " 'America/Danmarkshavn',\n",
       " 'America/Dawson',\n",
       " 'America/Dawson_Creek',\n",
       " 'America/Denver',\n",
       " 'America/Detroit',\n",
       " 'America/Dominica',\n",
       " 'America/Edmonton',\n",
       " 'America/Eirunepe',\n",
       " 'America/El_Salvador',\n",
       " 'America/Ensenada',\n",
       " 'America/Fort_Nelson',\n",
       " 'America/Fort_Wayne',\n",
       " 'America/Fortaleza',\n",
       " 'America/Glace_Bay',\n",
       " 'America/Godthab',\n",
       " 'America/Goose_Bay',\n",
       " 'America/Grand_Turk',\n",
       " 'America/Grenada',\n",
       " 'America/Guadeloupe',\n",
       " 'America/Guatemala',\n",
       " 'America/Guayaquil',\n",
       " 'America/Guyana',\n",
       " 'America/Halifax',\n",
       " 'America/Havana',\n",
       " 'America/Hermosillo',\n",
       " 'America/Indiana/Indianapolis',\n",
       " 'America/Indiana/Knox',\n",
       " 'America/Indiana/Marengo',\n",
       " 'America/Indiana/Petersburg',\n",
       " 'America/Indiana/Tell_City',\n",
       " 'America/Indiana/Vevay',\n",
       " 'America/Indiana/Vincennes',\n",
       " 'America/Indiana/Winamac',\n",
       " 'America/Indianapolis',\n",
       " 'America/Inuvik',\n",
       " 'America/Iqaluit',\n",
       " 'America/Jamaica',\n",
       " 'America/Jujuy',\n",
       " 'America/Juneau',\n",
       " 'America/Kentucky/Louisville',\n",
       " 'America/Kentucky/Monticello',\n",
       " 'America/Knox_IN',\n",
       " 'America/Kralendijk',\n",
       " 'America/La_Paz',\n",
       " 'America/Lima',\n",
       " 'America/Los_Angeles',\n",
       " 'America/Louisville',\n",
       " 'America/Lower_Princes',\n",
       " 'America/Maceio',\n",
       " 'America/Managua',\n",
       " 'America/Manaus',\n",
       " 'America/Marigot',\n",
       " 'America/Martinique',\n",
       " 'America/Matamoros',\n",
       " 'America/Mazatlan',\n",
       " 'America/Mendoza',\n",
       " 'America/Menominee',\n",
       " 'America/Merida',\n",
       " 'America/Metlakatla',\n",
       " 'America/Mexico_City',\n",
       " 'America/Miquelon',\n",
       " 'America/Moncton',\n",
       " 'America/Monterrey',\n",
       " 'America/Montevideo',\n",
       " 'America/Montreal',\n",
       " 'America/Montserrat',\n",
       " 'America/Nassau',\n",
       " 'America/New_York',\n",
       " 'America/Nipigon',\n",
       " 'America/Nome',\n",
       " 'America/Noronha',\n",
       " 'America/North_Dakota/Beulah',\n",
       " 'America/North_Dakota/Center',\n",
       " 'America/North_Dakota/New_Salem',\n",
       " 'America/Ojinaga',\n",
       " 'America/Panama',\n",
       " 'America/Pangnirtung',\n",
       " 'America/Paramaribo',\n",
       " 'America/Phoenix',\n",
       " 'America/Port-au-Prince',\n",
       " 'America/Port_of_Spain',\n",
       " 'America/Porto_Acre',\n",
       " 'America/Porto_Velho',\n",
       " 'America/Puerto_Rico',\n",
       " 'America/Punta_Arenas',\n",
       " 'America/Rainy_River',\n",
       " 'America/Rankin_Inlet',\n",
       " 'America/Recife',\n",
       " 'America/Regina',\n",
       " 'America/Resolute',\n",
       " 'America/Rio_Branco',\n",
       " 'America/Rosario',\n",
       " 'America/Santa_Isabel',\n",
       " 'America/Santarem',\n",
       " 'America/Santiago',\n",
       " 'America/Santo_Domingo',\n",
       " 'America/Sao_Paulo',\n",
       " 'America/Scoresbysund',\n",
       " 'America/Shiprock',\n",
       " 'America/Sitka',\n",
       " 'America/St_Barthelemy',\n",
       " 'America/St_Johns',\n",
       " 'America/St_Kitts',\n",
       " 'America/St_Lucia',\n",
       " 'America/St_Thomas',\n",
       " 'America/St_Vincent',\n",
       " 'America/Swift_Current',\n",
       " 'America/Tegucigalpa',\n",
       " 'America/Thule',\n",
       " 'America/Thunder_Bay',\n",
       " 'America/Tijuana',\n",
       " 'America/Toronto',\n",
       " 'America/Tortola',\n",
       " 'America/Vancouver',\n",
       " 'America/Virgin',\n",
       " 'America/Whitehorse',\n",
       " 'America/Winnipeg',\n",
       " 'America/Yakutat',\n",
       " 'America/Yellowknife',\n",
       " 'Antarctica/Casey',\n",
       " 'Antarctica/Davis',\n",
       " 'Antarctica/DumontDUrville',\n",
       " 'Antarctica/Macquarie',\n",
       " 'Antarctica/Mawson',\n",
       " 'Antarctica/McMurdo',\n",
       " 'Antarctica/Palmer',\n",
       " 'Antarctica/Rothera',\n",
       " 'Antarctica/South_Pole',\n",
       " 'Antarctica/Syowa',\n",
       " 'Antarctica/Troll',\n",
       " 'Antarctica/Vostok',\n",
       " 'Arctic/Longyearbyen',\n",
       " 'Asia/Aden',\n",
       " 'Asia/Almaty',\n",
       " 'Asia/Amman',\n",
       " 'Asia/Anadyr',\n",
       " 'Asia/Aqtau',\n",
       " 'Asia/Aqtobe',\n",
       " 'Asia/Ashgabat',\n",
       " 'Asia/Ashkhabad',\n",
       " 'Asia/Atyrau',\n",
       " 'Asia/Baghdad',\n",
       " 'Asia/Bahrain',\n",
       " 'Asia/Baku',\n",
       " 'Asia/Bangkok',\n",
       " 'Asia/Barnaul',\n",
       " 'Asia/Beirut',\n",
       " 'Asia/Bishkek',\n",
       " 'Asia/Brunei',\n",
       " 'Asia/Calcutta',\n",
       " 'Asia/Chita',\n",
       " 'Asia/Choibalsan',\n",
       " 'Asia/Chongqing',\n",
       " 'Asia/Chungking',\n",
       " 'Asia/Colombo',\n",
       " 'Asia/Dacca',\n",
       " 'Asia/Damascus',\n",
       " 'Asia/Dhaka',\n",
       " 'Asia/Dili',\n",
       " 'Asia/Dubai',\n",
       " 'Asia/Dushanbe',\n",
       " 'Asia/Famagusta',\n",
       " 'Asia/Gaza',\n",
       " 'Asia/Harbin',\n",
       " 'Asia/Hebron',\n",
       " 'Asia/Ho_Chi_Minh',\n",
       " 'Asia/Hong_Kong',\n",
       " 'Asia/Hovd',\n",
       " 'Asia/Irkutsk',\n",
       " 'Asia/Istanbul',\n",
       " 'Asia/Jakarta',\n",
       " 'Asia/Jayapura',\n",
       " 'Asia/Jerusalem',\n",
       " 'Asia/Kabul',\n",
       " 'Asia/Kamchatka',\n",
       " 'Asia/Karachi',\n",
       " 'Asia/Kashgar',\n",
       " 'Asia/Kathmandu',\n",
       " 'Asia/Katmandu',\n",
       " 'Asia/Khandyga',\n",
       " 'Asia/Kolkata',\n",
       " 'Asia/Krasnoyarsk',\n",
       " 'Asia/Kuala_Lumpur',\n",
       " 'Asia/Kuching',\n",
       " 'Asia/Kuwait',\n",
       " 'Asia/Macao',\n",
       " 'Asia/Macau',\n",
       " 'Asia/Magadan',\n",
       " 'Asia/Makassar',\n",
       " 'Asia/Manila',\n",
       " 'Asia/Muscat',\n",
       " 'Asia/Nicosia',\n",
       " 'Asia/Novokuznetsk',\n",
       " 'Asia/Novosibirsk',\n",
       " 'Asia/Omsk',\n",
       " 'Asia/Oral',\n",
       " 'Asia/Phnom_Penh',\n",
       " 'Asia/Pontianak',\n",
       " 'Asia/Pyongyang',\n",
       " 'Asia/Qatar',\n",
       " 'Asia/Qyzylorda',\n",
       " 'Asia/Rangoon',\n",
       " 'Asia/Riyadh',\n",
       " 'Asia/Saigon',\n",
       " 'Asia/Sakhalin',\n",
       " 'Asia/Samarkand',\n",
       " 'Asia/Seoul',\n",
       " 'Asia/Shanghai',\n",
       " 'Asia/Singapore',\n",
       " 'Asia/Srednekolymsk',\n",
       " 'Asia/Taipei',\n",
       " 'Asia/Tashkent',\n",
       " 'Asia/Tbilisi',\n",
       " 'Asia/Tehran',\n",
       " 'Asia/Tel_Aviv',\n",
       " 'Asia/Thimbu',\n",
       " 'Asia/Thimphu',\n",
       " 'Asia/Tokyo',\n",
       " 'Asia/Tomsk',\n",
       " 'Asia/Ujung_Pandang',\n",
       " 'Asia/Ulaanbaatar',\n",
       " 'Asia/Ulan_Bator',\n",
       " 'Asia/Urumqi',\n",
       " 'Asia/Ust-Nera',\n",
       " 'Asia/Vientiane',\n",
       " 'Asia/Vladivostok',\n",
       " 'Asia/Yakutsk',\n",
       " 'Asia/Yangon',\n",
       " 'Asia/Yekaterinburg',\n",
       " 'Asia/Yerevan',\n",
       " 'Atlantic/Azores',\n",
       " 'Atlantic/Bermuda',\n",
       " 'Atlantic/Canary',\n",
       " 'Atlantic/Cape_Verde',\n",
       " 'Atlantic/Faeroe',\n",
       " 'Atlantic/Faroe',\n",
       " 'Atlantic/Jan_Mayen',\n",
       " 'Atlantic/Madeira',\n",
       " 'Atlantic/Reykjavik',\n",
       " 'Atlantic/South_Georgia',\n",
       " 'Atlantic/St_Helena',\n",
       " 'Atlantic/Stanley',\n",
       " 'Australia/ACT',\n",
       " 'Australia/Adelaide',\n",
       " 'Australia/Brisbane',\n",
       " 'Australia/Broken_Hill',\n",
       " 'Australia/Canberra',\n",
       " 'Australia/Currie',\n",
       " 'Australia/Darwin',\n",
       " 'Australia/Eucla',\n",
       " 'Australia/Hobart',\n",
       " 'Australia/LHI',\n",
       " 'Australia/Lindeman',\n",
       " 'Australia/Lord_Howe',\n",
       " 'Australia/Melbourne',\n",
       " 'Australia/NSW',\n",
       " 'Australia/North',\n",
       " 'Australia/Perth',\n",
       " 'Australia/Queensland',\n",
       " 'Australia/South',\n",
       " 'Australia/Sydney',\n",
       " 'Australia/Tasmania',\n",
       " 'Australia/Victoria',\n",
       " 'Australia/West',\n",
       " 'Australia/Yancowinna',\n",
       " 'Brazil/Acre',\n",
       " 'Brazil/DeNoronha',\n",
       " 'Brazil/East',\n",
       " 'Brazil/West',\n",
       " 'CET',\n",
       " 'CST6CDT',\n",
       " 'Canada/Atlantic',\n",
       " 'Canada/Central',\n",
       " 'Canada/Eastern',\n",
       " 'Canada/Mountain',\n",
       " 'Canada/Newfoundland',\n",
       " 'Canada/Pacific',\n",
       " 'Canada/Saskatchewan',\n",
       " 'Canada/Yukon',\n",
       " 'Chile/Continental',\n",
       " 'Chile/EasterIsland',\n",
       " 'Cuba',\n",
       " 'EET',\n",
       " 'EST',\n",
       " 'EST5EDT',\n",
       " 'Egypt',\n",
       " 'Eire',\n",
       " 'Etc/GMT',\n",
       " 'Etc/GMT+0',\n",
       " 'Etc/GMT+1',\n",
       " 'Etc/GMT+10',\n",
       " 'Etc/GMT+11',\n",
       " 'Etc/GMT+12',\n",
       " 'Etc/GMT+2',\n",
       " 'Etc/GMT+3',\n",
       " 'Etc/GMT+4',\n",
       " 'Etc/GMT+5',\n",
       " 'Etc/GMT+6',\n",
       " 'Etc/GMT+7',\n",
       " 'Etc/GMT+8',\n",
       " 'Etc/GMT+9',\n",
       " 'Etc/GMT-0',\n",
       " 'Etc/GMT-1',\n",
       " 'Etc/GMT-10',\n",
       " 'Etc/GMT-11',\n",
       " 'Etc/GMT-12',\n",
       " 'Etc/GMT-13',\n",
       " 'Etc/GMT-14',\n",
       " 'Etc/GMT-2',\n",
       " 'Etc/GMT-3',\n",
       " 'Etc/GMT-4',\n",
       " 'Etc/GMT-5',\n",
       " 'Etc/GMT-6',\n",
       " 'Etc/GMT-7',\n",
       " 'Etc/GMT-8',\n",
       " 'Etc/GMT-9',\n",
       " 'Etc/GMT0',\n",
       " 'Etc/Greenwich',\n",
       " 'Etc/UCT',\n",
       " 'Etc/UTC',\n",
       " 'Etc/Universal',\n",
       " 'Etc/Zulu',\n",
       " 'Europe/Amsterdam',\n",
       " 'Europe/Andorra',\n",
       " 'Europe/Astrakhan',\n",
       " 'Europe/Athens',\n",
       " 'Europe/Belfast',\n",
       " 'Europe/Belgrade',\n",
       " 'Europe/Berlin',\n",
       " 'Europe/Bratislava',\n",
       " 'Europe/Brussels',\n",
       " 'Europe/Bucharest',\n",
       " 'Europe/Budapest',\n",
       " 'Europe/Busingen',\n",
       " 'Europe/Chisinau',\n",
       " 'Europe/Copenhagen',\n",
       " 'Europe/Dublin',\n",
       " 'Europe/Gibraltar',\n",
       " 'Europe/Guernsey',\n",
       " 'Europe/Helsinki',\n",
       " 'Europe/Isle_of_Man',\n",
       " 'Europe/Istanbul',\n",
       " 'Europe/Jersey',\n",
       " 'Europe/Kaliningrad',\n",
       " 'Europe/Kiev',\n",
       " 'Europe/Kirov',\n",
       " 'Europe/Lisbon',\n",
       " 'Europe/Ljubljana',\n",
       " 'Europe/London',\n",
       " 'Europe/Luxembourg',\n",
       " 'Europe/Madrid',\n",
       " 'Europe/Malta',\n",
       " 'Europe/Mariehamn',\n",
       " 'Europe/Minsk',\n",
       " 'Europe/Monaco',\n",
       " 'Europe/Moscow',\n",
       " 'Europe/Nicosia',\n",
       " 'Europe/Oslo',\n",
       " 'Europe/Paris',\n",
       " 'Europe/Podgorica',\n",
       " 'Europe/Prague',\n",
       " 'Europe/Riga',\n",
       " 'Europe/Rome',\n",
       " 'Europe/Samara',\n",
       " 'Europe/San_Marino',\n",
       " 'Europe/Sarajevo',\n",
       " 'Europe/Saratov',\n",
       " 'Europe/Simferopol',\n",
       " 'Europe/Skopje',\n",
       " 'Europe/Sofia',\n",
       " 'Europe/Stockholm',\n",
       " 'Europe/Tallinn',\n",
       " 'Europe/Tirane',\n",
       " 'Europe/Tiraspol',\n",
       " 'Europe/Ulyanovsk',\n",
       " 'Europe/Uzhgorod',\n",
       " 'Europe/Vaduz',\n",
       " 'Europe/Vatican',\n",
       " 'Europe/Vienna',\n",
       " 'Europe/Vilnius',\n",
       " 'Europe/Volgograd',\n",
       " 'Europe/Warsaw',\n",
       " 'Europe/Zagreb',\n",
       " 'Europe/Zaporozhye',\n",
       " 'Europe/Zurich',\n",
       " 'GB',\n",
       " 'GB-Eire',\n",
       " 'GMT',\n",
       " 'GMT+0',\n",
       " 'GMT-0',\n",
       " 'GMT0',\n",
       " 'Greenwich',\n",
       " 'HST',\n",
       " 'Hongkong',\n",
       " 'Iceland',\n",
       " 'Indian/Antananarivo',\n",
       " 'Indian/Chagos',\n",
       " 'Indian/Christmas',\n",
       " 'Indian/Cocos',\n",
       " 'Indian/Comoro',\n",
       " 'Indian/Kerguelen',\n",
       " 'Indian/Mahe',\n",
       " 'Indian/Maldives',\n",
       " 'Indian/Mauritius',\n",
       " 'Indian/Mayotte',\n",
       " 'Indian/Reunion',\n",
       " 'Iran',\n",
       " 'Israel',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Kwajalein',\n",
       " 'Libya',\n",
       " 'MET',\n",
       " 'MST',\n",
       " 'MST7MDT',\n",
       " 'Mexico/BajaNorte',\n",
       " 'Mexico/BajaSur',\n",
       " 'Mexico/General',\n",
       " 'NZ',\n",
       " 'NZ-CHAT',\n",
       " 'Navajo',\n",
       " 'PRC',\n",
       " 'PST8PDT',\n",
       " 'Pacific/Apia',\n",
       " 'Pacific/Auckland',\n",
       " 'Pacific/Bougainville',\n",
       " 'Pacific/Chatham',\n",
       " 'Pacific/Chuuk',\n",
       " 'Pacific/Easter',\n",
       " 'Pacific/Efate',\n",
       " 'Pacific/Enderbury',\n",
       " 'Pacific/Fakaofo',\n",
       " 'Pacific/Fiji',\n",
       " 'Pacific/Funafuti',\n",
       " 'Pacific/Galapagos',\n",
       " 'Pacific/Gambier',\n",
       " 'Pacific/Guadalcanal',\n",
       " 'Pacific/Guam',\n",
       " 'Pacific/Honolulu',\n",
       " 'Pacific/Johnston',\n",
       " 'Pacific/Kiritimati',\n",
       " 'Pacific/Kosrae',\n",
       " 'Pacific/Kwajalein',\n",
       " 'Pacific/Majuro',\n",
       " 'Pacific/Marquesas',\n",
       " 'Pacific/Midway',\n",
       " 'Pacific/Nauru',\n",
       " 'Pacific/Niue',\n",
       " 'Pacific/Norfolk',\n",
       " 'Pacific/Noumea',\n",
       " 'Pacific/Pago_Pago',\n",
       " 'Pacific/Palau',\n",
       " 'Pacific/Pitcairn',\n",
       " 'Pacific/Pohnpei',\n",
       " 'Pacific/Ponape',\n",
       " 'Pacific/Port_Moresby',\n",
       " 'Pacific/Rarotonga',\n",
       " 'Pacific/Saipan',\n",
       " 'Pacific/Samoa',\n",
       " 'Pacific/Tahiti',\n",
       " 'Pacific/Tarawa',\n",
       " 'Pacific/Tongatapu',\n",
       " 'Pacific/Truk',\n",
       " 'Pacific/Wake',\n",
       " 'Pacific/Wallis',\n",
       " 'Pacific/Yap',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'ROC',\n",
       " 'ROK',\n",
       " 'Singapore',\n",
       " 'Turkey',\n",
       " 'UCT',\n",
       " 'US/Alaska',\n",
       " 'US/Aleutian',\n",
       " 'US/Arizona',\n",
       " 'US/Central',\n",
       " 'US/East-Indiana',\n",
       " 'US/Eastern',\n",
       " 'US/Hawaii',\n",
       " 'US/Indiana-Starke',\n",
       " 'US/Michigan',\n",
       " 'US/Mountain',\n",
       " 'US/Pacific',\n",
       " 'US/Samoa',\n",
       " 'UTC',\n",
       " 'Universal',\n",
       " 'W-SU',\n",
       " 'WET',\n",
       " 'Zulu']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all time zones\n",
    "from pytz import all_timezones\n",
    "all_timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Selecting Dates and TImes: you have a vector of dates and you want to select one or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8762</th>\n",
       "      <td>2002-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8763</th>\n",
       "      <td>2002-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8764</th>\n",
       "      <td>2002-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date\n",
       "8762 2002-01-01 02:00:00\n",
       "8763 2002-01-01 03:00:00\n",
       "8764 2002-01-01 04:00:00"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['date']=pd.date_range('1/1/2001',periods=100000,freq='H')\n",
    "df[(df['date']>\"2002-1-1 01:00:00\") & \n",
    "  (df['date']<='2002-1-1 04:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-01 01:00:00</th>\n",
       "      <td>2002-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 02:00:00</th>\n",
       "      <td>2002-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 03:00:00</th>\n",
       "      <td>2002-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 04:00:00</th>\n",
       "      <td>2002-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   date\n",
       "date                                   \n",
       "2002-01-01 01:00:00 2002-01-01 01:00:00\n",
       "2002-01-01 02:00:00 2002-01-01 02:00:00\n",
       "2002-01-01 03:00:00 2002-01-01 03:00:00\n",
       "2002-01-01 04:00:00 2002-01-01 04:00:00"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatively, we can set the date column as the DataFrame's index and then slice using loc\n",
    "\n",
    "df=df.set_index(df['date'])\n",
    "# select observaitons between two datetimes\n",
    "df.loc['2002-1-1 01:00:00':'2002-1-1 04:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Breaking Up Data into Multiple Features: you have a columns of dates and times and you want to create features for year, month, day, hour, and minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-07</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-14</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-21</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-28</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-02-04</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  year  month  day  hour  minute\n",
       "0 2001-01-07  2001      1    7     0       0\n",
       "1 2001-01-14  2001      1   14     0       0\n",
       "2 2001-01-21  2001      1   21     0       0\n",
       "3 2001-01-28  2001      1   28     0       0\n",
       "4 2001-02-04  2001      2    4     0       0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['date']=pd.date_range('1/1/2001',periods=150,freq='W')\n",
    "# create features for year, month, day, hour, and minute\n",
    "df['year']=df['date'].dt.year\n",
    "df['month']=df['date'].dt.month\n",
    "df['day']=df['date'].dt.day\n",
    "df['hour']=df['date'].dt.hour\n",
    "df['minute']=df['date'].dt.minute\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Calculating the Difference Between Dates: you have two datetime features and want to calculate the time between them for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Arrived       Left\n",
      "0 2017-04-01 2017-04-01\n",
      "1 2017-04-05 2017-04-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   0 days\n",
       "1   1 days\n",
       "dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['Arrived']=[pd.Timestamp('04-01-2017'),pd.Timestamp('04-05-2017')]\n",
    "df['Left']=[pd.Timestamp('04-01-2017'),pd.Timestamp('04-06-2017')]\n",
    "print(df)\n",
    "# calculate duration between features\n",
    "df['Left']-df['Arrived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# often we will want to remove the day output and keep only the numerical value\n",
    "\n",
    "pd.Series(delta.days for delta in (df['Left']-df['Arrived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Encoding Days of the Week: you have a vector of dates and want to know the day of the week for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Thursday\n",
       "1      Sunday\n",
       "2     Tuesday\n",
       "3      Friday\n",
       "4      Sunday\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dates=pd.Series(pd.date_range('2/2/2002',periods=5,freq=\"M\"))\n",
    "# show days of the week\n",
    "dates.dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    6\n",
       "2    1\n",
       "3    4\n",
       "4    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we c=want the output to be a numerical value and therefore more usable as a machine learning feature, we can use weekday where \n",
    "# the days of the week are represented as an integer( Monday is 0)\n",
    "\n",
    "dates.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Creating a Lagged Festure: want to create a featurer that is lagged  n time periods\n",
    "* use pandas' shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Using Rolling Time Windows： you want to calculate some statistics for a rolling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_price\n",
       "2010-01-31          NaN\n",
       "2010-02-28          1.5\n",
       "2010-03-31          2.5\n",
       "2010-04-30          3.5\n",
       "2010-05-31          4.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "time_index=pd.date_range('01/01/2010',periods=5,freq='M')\n",
    "df=pd.DataFrame(index=time_index)\n",
    "df['stock_price']=[1,2,3,4,5]\n",
    "# calculate rolling mean\n",
    "df.rolling(window=2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Handling MIssing Data in TIme series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------linear\n",
      "             sales\n",
      "2010-01-31    1.0\n",
      "2010-02-28    2.0\n",
      "2010-03-31    3.0\n",
      "2010-04-30    4.0\n",
      "2010-05-31    5.0\n",
      "\n",
      "------quadratic\n",
      "                sales\n",
      "2010-01-31  1.000000\n",
      "2010-02-28  2.000000\n",
      "2010-03-31  3.059808\n",
      "2010-04-30  4.038069\n",
      "2010-05-31  5.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "time_index=pd.date_range('01/01/2010',periods=5,freq='M')\n",
    "df=pd.DataFrame(index=time_index)\n",
    "df['sales']=[1,2,np.nan,np.nan,5]\n",
    "# interpolate missing values\n",
    "print('------linear\\n',df.interpolate())\n",
    "print('\\n------quadratic\\n',df.interpolate(method='quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales\n",
       "2010-01-31    1.0\n",
       "2010-02-28    2.0\n",
       "2010-03-31    2.0\n",
       "2010-04-30    2.0\n",
       "2010-05-31    5.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatively, we can replace the missing values with the last known value(forward-filling)\n",
    "\n",
    "df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales\n",
       "2010-01-31    1.0\n",
       "2010-02-28    2.0\n",
       "2010-03-31    5.0\n",
       "2010-04-30    5.0\n",
       "2010-05-31    5.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also replace missing values with the latest known value(back-filling)\n",
    "\n",
    "df.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>3.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sales\n",
       "2010-01-31  1.000000\n",
       "2010-02-28  2.000000\n",
       "2010-03-31  3.059808\n",
       "2010-04-30       NaN\n",
       "2010-05-31  5.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, there might be cases when we have large gaps of missing values and do not want to interpolate values across the entire gap. \n",
    "# In these cases we can use limit to restrict the number of interpolated values and limit_direction to set whether to interpolate values \n",
    "# forward from at the last known value before the gap or vice versa:\n",
    " \n",
    "df.interpolate(limit=1, limit_direction=\"forward\",method='quadratic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Handling Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 Dimensionality Reduction Using Festure _Extraction_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Reducing Features Using Principle Components: reduce the number of features while retaining the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of features: 64\n",
      "reduced number of features: 54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "digits=datasets.load_digits()\n",
    "# standardize the feature matrix\n",
    "features=StandardScaler().fit_transform(digits.data)\n",
    "# create a PCA that will retain 99% of variance\n",
    "pca=PCA(n_components=0.99,whiten=True)\n",
    "# conduct PCA\n",
    "features_pca=pca.fit_transform(features)\n",
    "# show results\n",
    "print('original number of features:', features.shape[1])\n",
    "print('reduced number of features:',features_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Reducing Features When Data is Linearly Inseparable: you suspect you have linearly inseparable data and want to reduce the dimensions\n",
    "* Use an extension of principal component analysis that uses kernels to allow for non-linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of features: 2\n",
      "reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "# create linearly inseparable data\n",
    "features,_=make_circles(n_samples=1000,random_state=1,noise=0.1,factor=0.1)\n",
    "# apply kernel PCA with radius basis function(RBF) kernel\n",
    "kpca=KernelPCA(kernel='rbf',gamma=15,n_components=1)\n",
    "features_kpca=kpca.fit_transform(features)\n",
    "\n",
    "print('original number of features:',features.shape[1])\n",
    "print('reduced number of features:',features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Reducing Features by Maximizing Class Separability: reduce the features to be used by a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "iris=datasets.load_iris()\n",
    "features=iris.data\n",
    "target=iris.target\n",
    "\n",
    "# create and run an LDA, then use it to transform the features\n",
    "lda=LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda=lda.fit(features,target).transform(features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_lda.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Reducing Feature Using Matrix Factorization: you have a feature matrix of nonnegative values and want to reduce the dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF # non-negative matrix factorization\n",
    "from sklearn import datasets\n",
    "digits=datasets.load_digits()\n",
    "features=digits.data\n",
    "# create , fit, and apply NMF\n",
    "nmf=NMF(n_components=10,random_state=1)\n",
    "features_nmf=nmf.fit_transform(features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Reducing Features on Sparse Data: you have a sparse matrix and want to reduce the dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 10\n"
     ]
    }
   ],
   "source": [
    "# use Truncated Singular Value Decomposition(TSVD)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "digits=datasets.load_digits()\n",
    "# standardize feature matrix\n",
    "features=StandardScaler().fit_transform(digits.data)\n",
    "# make sparse matrix\n",
    "features_sparse=csr_matrix(features)\n",
    "# create TSVD on sparse matrix\n",
    "tsvd=TruncatedSVD(n_components=10)\n",
    "# conduct TSVD on sparse matrix\n",
    "features_sparse_tsvd=tsvd.fit(features_sparse).transform(features_sparse)\n",
    "\n",
    "print(\"Original number of features:\", features_sparse.shape[1])\n",
    "print('Reduced number of features:', features_sparse_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 Dimensionality Reduction Using Feature _Selection_\n",
    "## 10.1 Thresholding Numerical Feature Variance: you have a set of numerical features and want to remove those with low variance(i.e., likely containing little information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Thresholding Binary Feature Variance: You have a set of binary categorical features and want to remove those with low variance (i.e., likely containing little information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create feature matrix with:\n",
    "# Feature 0: 80% class 0\n",
    "# Feature 1: 80% class 1\n",
    "# Feature 2: 60% class 0, 40% class 1\n",
    "features = [[0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]]\n",
    "\n",
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Handling Highly Correlated Features: You have a feature matrix and suspect some features are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                          k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Removing Irrelevant Features for Classification: You have a categorical target vector and want to remove uninformative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Convert to categorical data by converting data to integers\n",
    "features = features.astype(int)\n",
    "\n",
    "# Select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# If the features are quantitative, compute the ANOVA F-value between each feature and the target vector:\n",
    "\n",
    "# Select two features with highest F-values\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 3\n"
     ]
    }
   ],
   "source": [
    "# Instead of selecting a specific number of features, we can also use SelectPercentile to select the top n percent of features:\n",
    "\n",
    "# Load library\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Select top 75% of features with highest F-values\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Recursively Eliminating Features: You want to automatically select the best features to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  1.13220029,  0.7031277 , ..., -0.26903836,\n",
       "        -0.19448816,  1.5417181 ],\n",
       "       [-1.07500204,  1.02994989,  2.56148527, ..., -0.71990239,\n",
       "         1.24355995,  0.90054733],\n",
       "       [ 1.37940721, -1.58203529, -1.77039484, ...,  0.49558942,\n",
       "        -0.59421636,  1.23699023],\n",
       "       ...,\n",
       "       [-0.80331656, -0.3140609 , -1.60648007, ...,  0.22900133,\n",
       "         0.93401741,  0.07420836],\n",
       "       [ 0.39508844, -0.6374364 , -1.34564911, ...,  0.02723818,\n",
       "         0.89998868, -0.62569347],\n",
       "       [-0.55383035, -2.20671933,  0.82880112, ..., -1.08865284,\n",
       "         1.32131624,  1.46404781]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Suppress an annoying but harmless warning\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n",
    "                        message=\"^internal gelsd\")\n",
    "\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                   n_features = 100,\n",
    "                                   n_informative = 2,\n",
    "                                   random_state = 1)\n",
    "\n",
    "# Create a linear regression\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# Recursively eliminate features\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Model Evaluation\n",
    "## 11.1 Cross-Validation Models: you want to evaluate how well your model work in the real world "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964931719428926"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Create features matrix\n",
    "features = digits.data\n",
    "\n",
    "# Create target vector\n",
    "target = digits.target\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Create logistic regression object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "# Create k-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             features, # Feature matrix\n",
    "                             target, # Target vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU scores\n",
    "\n",
    "# Calculate mean\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Creating a Baseline Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
